{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yong', 'is', 'a', 'good', 'guy', ',', 'he', 'is', 'not', 'bad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davideliason/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davideliason/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/davideliason/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/davideliason/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# see Text-represent-code word doc in images folder\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "d1 = \"Yong is a good guy, he is not bad\"\n",
    "d2 = \"feet wolves cooked boys girls ,!<@!\"\n",
    "d3 = \"Yong is not a good guy, he is bad\"\n",
    "\n",
    "c1 = [d1, d2, d3]\n",
    "\n",
    "token_d1 = nltk.word_tokenize(d1)\n",
    "print(token_d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yong', 'is', 'a', 'good', 'guy', ',', 'he', 'is', 'not', 'bad']\n",
      "['Yong', 'is', 'a', 'good', 'guy,', 'he', 'is', 'not', 'bad']\n"
     ]
    }
   ],
   "source": [
    "token_d1 = nltk.word_tokenize(d1)\n",
    "print(token_d1)\n",
    "\n",
    "tokenizer2 = nltk.tokenize.WhitespaceTokenizer()\n",
    "token_d12 = tokenizer2.tokenize(d1)\n",
    "print(token_d12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yong': 11, 'is': 8, 'good': 5, 'guy': 6, 'he': 7, 'not': 9, 'bad': 0, 'feet': 3, 'wolves': 10, 'cooked': 2, 'boys': 1, 'girls': 4}\n",
      "[[1 0 0 0 0 1 1 1 2 1 0 1]\n",
      " [0 1 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 1 1 1 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# BOW\n",
    "##BOW Frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(c1)\n",
    "\n",
    "# these show the index not the frequency\n",
    "print(vectorizer1.vocabulary_)\n",
    "\n",
    "v1 = vectorizer1.transform(c1)\n",
    "print(v1.toarray())\n",
    "# this shows the frequency\n",
    "# there are three lines b/c there were 3 documents d1,d2,d3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feet', 'wolves', 'cooked', 'boys', 'girls', ',', '!', '<', '@', '!']\n",
      "['feet', 'wolv', 'cook', 'boy', 'girl']\n",
      "['feet', 'wolv', 'cook', 'boy', 'girl']\n",
      "['foot', 'wolf', 'cooked', 'boy', 'girl']\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "###STEMMER\n",
    "token_d2 = nltk.word_tokenize(d2.lower())\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmered_token_d2 = [stemmer.stem(token) for token in token_d2 if token.isalpha()]\n",
    "# isapha method is alphabetic only\n",
    "# punctuation not being considered\n",
    "\n",
    "#\n",
    "stemmed_token_d2 = []\n",
    "for token in token_d2:\n",
    "    if token.isalpha():\n",
    "        stemmed_token_d2.append(stemmer.stem(token))\n",
    "\n",
    "print(token_d2)\n",
    "print(stemmed_token_d2)\n",
    "print(stemmered_token_d2)\n",
    "\n",
    "#lemmatizer\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_token_d2 = [lemmatizer.lemmatize(token) for token in token_d2 if token.isalpha()]\n",
    "print(lemmatized_token_d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yong', 'is', 'a', 'good', 'guy', ',', 'he', 'is', 'not', 'bad']\n",
      "['Yong', 'good', 'guy', 'bad']\n"
     ]
    }
   ],
   "source": [
    "###remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_removed = [token for token in token_d1 if not token in stopwords.words('english') if token.isalpha()]\n",
    "\n",
    "print(token_d1)\n",
    "\n",
    "# these are the words with the stop words removed:\n",
    "print(stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yong': 6, 'is': 4, 'good': 1, 'guy': 2, 'he': 3, 'not': 5, 'bad': 0}\n",
      "[[1 1 1 1 2 1 1]\n",
      " [0 0 0 0 0 0 0]\n",
      " [1 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "### low frequency words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer(min_df=2)\n",
    "vectorizer1.fit(c1)\n",
    "print(vectorizer1.vocabulary_)\n",
    "v1 = vectorizer1.transform(c1)\n",
    "\n",
    "print(v1.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yong': 11, 'is': 8, 'good': 5, 'guy': 6, 'he': 7, 'not': 9, 'bad': 0, 'feet': 3, 'wolves': 10, 'cooked': 2, 'boys': 1, 'girls': 4}\n",
      "[[0.31622777 0.         0.         0.         0.         0.31622777\n",
      "  0.31622777 0.31622777 0.63245553 0.31622777 0.         0.31622777]\n",
      " [0.         0.4472136  0.4472136  0.4472136  0.4472136  0.\n",
      "  0.         0.         0.         0.         0.4472136  0.        ]\n",
      " [0.31622777 0.         0.         0.         0.         0.31622777\n",
      "  0.31622777 0.31622777 0.63245553 0.31622777 0.         0.31622777]]\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "###TFI-IDF\n",
    "#  Use a TF‐IDF to replace frequency in the term‐document matrix to represent the bag‐of‐words representation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(c1)\n",
    "print(vectorizer2.vocabulary_)\n",
    "\n",
    "v2 = vectorizer2.transform(c1)\n",
    "print(v2.toarray())\n",
    "\n",
    "c2 = [\"hello world\", \"Yong is calling\"]\n",
    "v_c2 = vectorizer2.transform(c2)\n",
    "print(v_c2.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25819889 0.25819889 0.25819889 0.25819889 0.25819889 0.25819889\n",
      "  0.25819889 0.51639778 0.25819889 0.25819889 0.25819889 0.25819889]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.25819889 0.25819889 0.25819889 0.25819889 0.25819889 0.25819889\n",
      "  0.25819889 0.51639778 0.25819889 0.25819889 0.25819889 0.25819889]]\n",
      "{'yong': 10, 'is': 7, 'good': 1, 'guy': 3, 'he': 5, 'not': 9, 'bad': 0, 'yong is': 11, 'good guy': 2, 'guy he': 4, 'he is': 6, 'is not': 8}\n"
     ]
    }
   ],
   "source": [
    "### Bag of 2-grams\n",
    "vectorizer3 = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "vectorizer3.fit(c1)\n",
    "v3 = vectorizer3.transform(c1)\n",
    "\n",
    "print(v3.toarray())\n",
    "print(vectorizer3.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iprp': 5, 'drinkvbp': 2, 'waternn': 7, 'inin': 4, 'partiesnns': 6, 'grabvbp': 3, 'adt': 0, 'drinknn': 1}\n",
      "[[0.         0.         0.53309782 0.         0.37930349 0.37930349\n",
      "  0.37930349 0.53309782]\n",
      " [0.47042643 0.47042643 0.         0.47042643 0.33471228 0.33471228\n",
      "  0.33471228 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "### POS TAG\n",
    "d4 = \"I drink water in parties\"\n",
    "d5 = \"I grab a drink in parties\"\n",
    "token4 = nltk.word_tokenize(d4)\n",
    "\n",
    "POS_token4 = nltk.pos_tag(token4)\n",
    "\n",
    "c2 = [d4, d5]\n",
    "POS_c2 = []\n",
    "for doc in c2:\n",
    "    token_doc = nltk.word_tokenize(doc)\n",
    "    POS_token_doc = nltk.pos_tag(token_doc)\n",
    "    POS_token_temp = []\n",
    "    for i in POS_token_doc:\n",
    "        POS_token_temp.append(i[0] + i[1])\n",
    "    POS_c2.append(\" \".join(POS_token_temp))\n",
    "\n",
    "vectorizer4 = TfidfVectorizer()\n",
    "vectorizer4.fit(POS_c2)\n",
    "print(vectorizer4.vocabulary_)\n",
    "\n",
    "POS_v3 = vectorizer4.transform(POS_c2)\n",
    "print(POS_v3.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
